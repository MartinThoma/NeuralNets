Neural Networks - Formulae Collection
=====================================

This is my personal collection of formulae in the field of neural networks. I started it in preparation to my exam on the neural nets at Karlsruhe Institute of Technology (KIT). Even so the content will be similar to the course it is neither limited to it nor linked with the course in any means. __This is not official script and may contain errors and lag completeness.__

Corrections, supplements (or wishes) and links to good sources/ papers are very welcome. Just mail me to marvin.ritter@gmail.com.

ToDo's
------
- Compare BGD, SGD and SGD with minibatches
- Overview of Pattern recognition (graphic + examples)
- Add useful literature 
	- [ ] books
	- [ ] papers
	- [ ] Internet tutorials/ demos
- Explain overfitting and underfitting (what is it, how to detect, how to prevent)
- Hyperparamaters \Phi (examples, how to train? => development set)
- DA
- RBM
- Differences between: stacked denoising autoencoders, stacked sparse autoencoders, stacked restricted boltzmann machine
- What is Curriculum Learning?
- Dropout
- Maxout
- Chapter on common datasets/ benchmarks
	- MNIST
	- CIFAR-10: 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
- Overview off network architectures (feed forward networks: perceptron, MLP, DNN; shared parameters: TDNN, CNN, RNN)
- Conjugate gradient method
- What is the Schwarz Criterion for k-Means
- Optimal Brain Damage
- Hebbian learning rule
- Practical Techniques for Improving Backpropagation (see slides and Pattern Classification Chapter 6.8)
- NewBob
- Quickprop (Pattern Recognition 6.9.3)

Open Questions
--------------
- Backpropagation requires the activation function to be differentiable. How can be use non differentiable activation functions, e. g. step function?
- Why is LBG better than k-Means?

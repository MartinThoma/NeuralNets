Neural Networks - Formulae Collection
=====================================

This is my personal collection of formulae in the field of neural networks. I started it in preparation to my exam on the neural nets at Karlsruhe Institute of Technology (KIT). Even so the content will be similar to the course it is neither limited to it nor linked with the course in any means. __This is not official script and may contain errors and lag completeness.__

Corrections, supplements (or wishes) and links to good sources/ papers are very welcome. Just mail me to marvin.ritter@gmail.com.

ToDo's
------
- BGD vs SGD vs SGD with minibatches
- overview of ML fields with example algorithms
- overfitting and underfitting (detect and prevont)
- Hyperparamaters \Phi (examples, how to train? => development set)
- DA
- RBM
- differences between: stacked denoising autoencoders, stacked sparse autoencoders, stacked restricted boltzmann machine
- what is Curriculum Learning?
- Dropout
- Maxout
- common datasets/ benchmarks
	- MNIST
	- CIFAR-10: 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
- overview off all architectures (feed forward networks: perceptron, mlp, dnn; shared parameters: TDNN = Convolutional NN, recurrent neural networks) - What are conjugate gradients, Levenberg-Marquardt, etc.?


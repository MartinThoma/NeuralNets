%!TEX root = ../NeuralNets.tex
\section{Data Normalization}\label{sec:data-normalization}

Numerical needs to be normalized because \gls{NN} function best with inputs
between in range $[0,1]$ or $[-1,+1]$. There are two common normalizations, the
min-max normalization (sometimes called feature scaling),
\begin{equation}\label{eq:min_max_normalization}
\hat x_i = \frac{x_i - \min(x)}{\max(x) - \min(x)}
\end{equation}
, which will transform the smallest value to $0$ and the biggest to $1$ and everything else linearly in between, and Gaussian normalization,
\begin{equation}\label{eq:gaussian_normalization}
\hat x_i  = \frac{x_i - \text{mean}(x)}{\text{std}(x)} = \frac{x_i - \mu}{\sigma}
\end{equation}
, will transform $x$ to have zero mean and a standard deviation of one. Other names for int are standard score or z-scores.

Those are the two most common methods, but depending on the input there might be more.
%!TEX root = ../NeuralNets.tex
\chapter{Autoencoders}\label{chapter:autoencoders}\xindex{autoencoder}%

\begin{figure}
\centering
\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
% neurons
\foreach \m [count=\y] in {1,2,3,4}
	\node[neuron, neuron-input, neuron-\m/.try ] (input-\m) at (0,2-\y*1.25) {};
\foreach \m [count=\y] in {1,2,3}
	\node[neuron, neuron-hidden, neuron-\m/.try ] (hidden-\m) at (2,1.3-\y*1.25) {};
\foreach \m [count=\y] in {1,2,3,4}
	\node[neuron, neuron-output, neuron-\m/.try ] (output-\m) at (4,2-\y*1.25) {};

% neuron labels
\foreach \l [count=\i] in {1,...,4}
	\node[above] at (input-\i.north) {$x_{\l}$};
\foreach \l [count=\i] in {1,2,3}
	\node[above] at (hidden-\i.north) {$y_{\l}$};
\foreach \l [count=\i] in {1,...,4}
	\node[above] at (output-\i.north) {$z_{\l}$};

% neuron connection
\foreach \i in {1,...,4}
	\foreach \j in {1,...,3}
		\draw[->] (input-\i) -- (hidden-\j);
\foreach \i in {1,...,3}
	\foreach \j in {1,...,4}
		\draw[->] (hidden-\i) -- (output-\j);

% layer labels
\foreach \l [count=\x from 0] in {Input, Hidden, Output}
	\node[align=center, above] at (\x*2,2) {\l \\ Layer};

\end{tikzpicture}
\caption{Simple Autoencoder}
\label{fig:autoencoder}
\end{figure}

\begin{itemize}
    \item Hidden layer can be of any size
    \item Output layer must have size of input layer
    \item Weight matrix $\mathbf{W}$ and bias $\mathbf{b}$ can be learning
          using backpropagation
    \item $\mathbf{W}^{(2)} = (\mathbf{W}^{(1)})^T$
    \item Non-linear activation function is required, otherwise autoencoder
          might learn pass through and hidden layer could be similar to
          \gls{PCA}
\end{itemize}

\section{Sparse Autoencoders}\label{sec:sparse-ae}\xindex{autoencoder!sparse}%
Literature: \cite{Ng2011}

Favor sparse weight matrices by forcing the average neuron activation $\hat\rho_j$ to be close to $\rho ~ 0.2$.
\begin{align}
\hat\rho_j = \frac{1}{X} \sum_{x\in X} \mathbf{w}_j \mathbf{x}\label{eq:rho_j}
\intertext{This can be done by adding the Kullback–Leibler divergence,}
KL(\rho||\hat \rho_j) = \rho \log{\frac{\rho}{\hat \rho_j}} + (1-\rho) \log{\frac{1-\rho}{1-\hat \rho_j}}\label{eq:kl-divergence}
\intertext{, to the error function $E$,}
E_{\text{sparse}} = E + \sum_{j\in\text{hidden}} KL(\rho||\hat\rho_j)
\end{align}
\Cref{fig:kl-divergence} shows the error introduced by the average neuron
activation of a single hidden neuron.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
	axis lines = middle,
	xlabel = $x$,
	ylabel = $y$,
	ymin = -1,
	ymax = 5,
	xmin = -0.5,
	xmax = 1.5,
	clip = false,
]
\addplot[
	domain = -0.0:0.99,
	samples = 1000,
	color = blue,
	thick, smooth,
	]
{0.2*ln(0.2/x)+0.8*ln(0.8/(1-x))}
node[above,pos=0.3] {$KL(0.2 || x)$};
\end{axis}
\end{tikzpicture}
\caption{Kullback–Leibler divergence $KL(\rho||\hat\rho_j)$ with $\rho=0.2$}
\label{fig:kl-divergence}
\end{figure}

\section{Denoising Autoencoders}\label{sec:da}\xindex{autoencoder!denoising}%
\begin{itemize}
    \item Create corrupted $X'$ of training data $X$, \eg using Gaussian noise
          or by setting values to zero
    \item During training use $X'$ as input, but target $X$ as output
    \item Resulting autoencoder finds good features for noisy input
\end{itemize}

\section{Stacking Autoencoders}\label{sec:stacked-ae}\xindex{autoencoder!stacking}%
\begin{itemize}
    \item Train first autoencoder $\text{AE}_1$ using the original training
          data $X$
    \item Input $X$ into $\text{AE}_1$ and use output as $X'$
    \item Train second autoencoder $\text{AE}_2$ using $X'$
    \item Use $\text{AE}_2$ to transform $X'$ to $X''$
    \item Continue to create $\text{EA}_3, \text{EA}_4, \cdots$
    \item Use weight matrices and biases from autoencoders to stack a \gls{DNN}
\end{itemize}

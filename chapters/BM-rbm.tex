\section{Restricted Boltzmann Machines}\label{sec:rbm}\xindex{Boltzmann Machine!Restricted}%
Literature: \cite{Bengio2009}

For an efficient training Boltzmann machines can be restricted to have a
bipartite graph with one set of visible neurons and on set of hidden neurons.
As shown in \cref{fig:rbm} there are only visible-hidden and
hidden-visible connection (still symmetric). Therefore hidden units $h_j$ only
depend on the visible units $v_j$ and vice versa.
\begin{align}
p(h_j=1|v) = \sigma(c_j + \sum_i w_{ij} v_i)\\
p(v_j=1|h) = \sigma(b_j + \sum_i w_{ij} h_i)
\end{align}
With $b_j$ as the biases for the visible units and $c_j$ for the hidden units.

\begin{figure}
\centering
\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
% neurons
\foreach \m [count=\y] in {1,2,missing,3}
	\node[neuron, neuron-input, neuron-\m/.try ] (visible-\m) at (0,2-\y*1.25) {};
\foreach \m [count=\y] in {1,2,3,missing,4}
	\node[neuron, neuron-hidden, neuron-\m/.try, ] (hidden-\m) at (2,2.3-\y*1.25) {};

% neuron labels
\foreach \l [count=\i] in {1,2,...,2,m}
	\node[above] at (visible-\i.north) {$X_{\l}$};
\foreach \l [count=\i] in {1,2,3,...,3,h}
	\node[above] at (hidden-\i.north) {$H_{\l}$};

% connections
\foreach \i in {1,...,3}
	\foreach \j in {1,...,4}
		\draw[->] (visible-\i) -- (hidden-\j);

% layer labels
\foreach \l [count=\x from 0] in {Visible, Hidden}
	\node[align=center, above] at (\x*2,2) {\l \\ Layer};

\end{tikzpicture}
\caption{Architecture of a \gls{RBM}}
\label{fig:rbm}
\end{figure}

\subsection{Energy}
\begin{equation}\label{eq:rbm-energy}
E(v,h) = - \sum_{i,j} w_{ij} v_i h_j - \sum_i a_i v_i - \sum_i b_i h_i
\end{equation}

\section{Training}
Literature: \cite{Hinton2012}

% \begin{equation}
% \Delta w_{ij} = \eta\,\frac{\partial \log p(v)}{\partial w_{ij}}
% \end{equation}

The most common algorithm used is \gls{CD} used inside a gradient-descent and preforming Gibbs Sampling. A single-step contrastive divergence (CD-1) procedure for a single training example can be summarized as follows:
\begin{enumerate}
% \item Take a training sample $v$, compute the probabilities of the hidden units and sample a hidden activation vector $h$ from this probability distribution.
% \item Compute the outer product of $v$ and $h$ and call this the \emph{positive gradient}.
% \item From $h$, sample a reconstruction $v'$ of the visible units, then resample the hidden activations $h'$ from this. (Gibbs sampling step)
% \item Compute the outer product of $v'$ and $h'$ and call this the \emph{negative gradient}.
% \item Let the weight update to $w_{ij}$ be the positive gradient minus the negative gradient, times some learning rate: $\Delta w_{ij} = \eta\, (\mathbf{v} \mathbf{h}^T - \mathbf{v}' \mathbf{h}^{'T})$.

\item Sample hidden units $\mathbf{h}$ from training example $\mathbf{v}$
\item Sample reconstruction $\mathbf{v}'$ of visible units using $\mathbf{h}$ and then resample $\mathbf{h}'$ from it. (Gibbs sampling step)
\item $w_{ij} \leftarrow w_{ij} - \eta\, (\mathbf{v} \mathbf{h}^T - \mathbf{v}' \mathbf{h}^{'T})$
\end{enumerate}

\section{Stacking RBMs}
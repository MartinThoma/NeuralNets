\section{k-Means}
Finding $C^*(i)$ by enumeration is too time-consuming. Instead use iterative greedy descent which leads to a local optima.
The loss function is defined as
\begin{equation}
J = \sum_{k=1}^K \sum_{C(i)=k} d(x_i, \mu_k)
\end{equation}

Using the squared Euclidean distance, $d(x_i, \mu_k) = ||x_i - \mu_k||^2$, will effectively assign each sample to the closet center.

A slightly different definition is to minimize our already defined $W(C)$
\begin{equation}
W(C) = \frac{1}{2} \sum\limits_{k=1}^K \sum\limits_{C(i)=k} \sum\limits_{C(j)=k} ||x_i - x_j||^2 = \sum\limits_{k=1}^K N_k \sum\limits_{C(i)=k} ||x_i - \mu_k||^2
\end{equation}

Also called \emph{Lloyd's Algorithm} we optimize for $J$ in a greedy fashion.
\begin{enumerate}
\item \textbf{Classify}: Assign each observation $i$ to the nearest centroid: $$C(i) = \argmin\limits_{1\leq k \leq K} ||x_i - \mu_k||^2$$
\item \textbf{Recenter}: For each class $k$, compute a new centroid as the mean of the updated class assignments: $$\mu_k = \frac{\sum\limits_{C(i)=k} x_i}{N_k}$$
\item \textbf{Repeat step 1 and 2 until stopping criteria fulfilled (\eg centers stop moving/ C(i) unchanged).}
\end{enumerate}

During the course of the k-means algorithm, the loss function monotonically decreases (in both steps) into a local minimum.

\section{Fuzzy k-Means}\label{sec:fuzzy-kmeans}
Literature: \cite{Introduction2000}

Observation does not belong strictly to one cluster, but has a member ship degree $u_{ik}$ for each cluster $k$.

TODO: Do we need $\sum_{k=1}^K u_{ik} = 1$ ?

Our new loss function is:
\begin{equation}
J_m = \sum_{i=1}^N \sum_{k=1}^K u_{ik} d(x_i, \mu_k)
\end{equation}

\begin{enumerate}
\item \textbf{Compute degree of membership:}
	$$u_{ik} = \left[ \sum_{j=1}^K \left( \frac{d(x_i, \mu_k)}{d(\mu_j, \mu_k)} \right)^{\frac{2}{m-1}} \right]^{-1}$$
\item \textbf{Recenter:}
	$$\mu_k = \frac{\sum_{i=1}^N u_{ik}^m \, x_i}{\sum_{i=1}^N u_{ik}^m}$$
\item \textbf{Repeat step 1 and 2 until stopping criteria fulfilled.}
\end{enumerate}

\section{LBG-Algorithm}
Original Paper: \cite{Linde1980}\\
Demo: \href{http://www.data-compression.com/vqanim.shtml}{external website}

The Linde, Buzo, Grey (LBG) Algorithm is an alternative method to design a $K$-vector codebook, where $K=2^x$.\\
The algorithm takes a parameter a small $\epsilon > 0$ (\eg $\epsilon=0.001$).

\begin{enumerate}
\item \textbf{Initialize} 1-vector starting codebook:
	\begin{align}
	\mu_1(0) &= \frac{1}{N} \sum\limits_{i=1}^N x_i\\
	t &= 0
	\end{align}
\item \textbf{Double codebook}:
	\begin{align}
	\mu_j(t+1) &= (1+\epsilon)\, \mu_j(t) \quad \text{for } j=1,\cdots,2^t\\
	\mu_{2*j+1}(t+1) &= (1-\epsilon)\, \mu_j(t) \quad \text{for } j=1,\cdots,2^t\\
	t &= t+1
	\end{align}
\item \textbf{Run k-Means} Use k-Means to optimize to improve the $\mu_j(t)$.
\item \textbf{Repeat step 2 and 3} until desired number of codebook vectors is reached (for $t=1,\cdots,\log_2(K)-1$)
\end{enumerate}

Our final codebook will be $\mu_j(\log_2(K))$.
%!TEX root = ../NeuralNets.tex
\section{Error Functions}\label{sec:error-functions}
Sometimes also called objective functions or loss-functions.

\subsection{Mean-Squared Error}\label{sec:mse}
As the name suggests the \gls{MSE} is defined as the mean (over all training
examples) of squared difference between the correct value $t_i$ and the correct
value $o_i$. This big errors are punished harder than small differences.
\begin{equation}\label{eq:mse}
E_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^n (\mathbf{t}_i - \mathbf{o}_i)^2
\end{equation}
This would derive to
\begin{align}
\begin{split}
\frac{\partial E_{\text{MSE}}}{\partial \mathbf{o}_j}
&= \frac{1}{n} \sum_{i=1}^n \frac{\partial (\mathbf{t}_i - \mathbf{o}_i)^2}{\partial \mathbf{o}_j}\\
&= \frac{1}{n} \frac{\partial (\mathbf{t}_j - \mathbf{o}_j)^2}{\partial \mathbf{o}_j}\\
&= \frac{1}{n} 2 (\mathbf{t}_j - \mathbf{o}_j) (-1)\\
&= \frac{2}{n} (\mathbf{o}_j - \mathbf{t}_j)
\end{split}
\end{align}
which is technically fine, but as the fraction is only a constant factor, we will often use a a slightly different definition:
\begin{align}\label{ed:mse2}
E_{\text{MSE}} &= \half \sum_{i=1}^k (t_i - o_i)^2\\
\frac{\partial E_{\text{MSE}}}{\partial o_j}
&= - (t_j - o_j) = o_j - t_j
\end{align}

\subsection{Cross Entropy Error}\label{sec:ce}
\Gls{CE} works great well on classifications tasks. $t_i$ is either $0$ or $1$ and $o_i$ is the class probability computed by the network $\Rightarrow o_i \in (0, 1]$ as $log(0)$ is not defined.
\begin{equation}\label{eq:ce}
E_{\text{CE}} = - \sum_{i=1}^k \left( t_i \log(o_i) + (1 - t_i) \log(1 - o_i) \right)
\end{equation}
This will derive to:
\begin{align}
\begin{split}
\frac{\partial E_{\text{CE}}}{\partial o_j} &= - \frac{\partial}{\partial o_j} \sum_{i=1}^k \left( t_i \log(o_i) + (1 - t_i) \log(1 - o_i) \right)\\
&= - \frac{\partial}{\partial o_j} t_j \log(o_j) - \frac{\partial}{\partial o_j} (1 - t_j) \log(1 - o_j)\\
&= - \frac{t_j}{o_j} + \frac{1 - t_j}{1 - o_j}\\
&= \frac{1 - t_j}{1 - o_j} - \frac{t_j}{o_j}
\end{split}
\end{align}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
	axis lines = middle,
	xlabel = $o_j$,
	ylabel = $E$,
	ymin = -1,
	ymax = 5,
	xmin = -0.5,
	xmax = 1.5,
	clip = false,
]
\addplot[
	domain = 0:0.99,
	samples = 100, 
	color = orange,
	thick, smooth,
	]
{-ln(x)}
node[left,pos=0] {$-\ln(o_j)$};
\addplot[
	domain = 0.01:1,
	samples = 100, 
	color = red,
	thick, smooth,
	]
{-ln(1-x)}
node[right,pos=1] {$-\ln(1-o_j)$};	
\end{axis}
\end{tikzpicture}
\caption{Error produced for $t_j=1$ (orange) and for $t_j=0$ (red). $o_j$ must be in range $(0,1]$, otherwise the logarithm can be complex or undefined or the error gets ridiculously large.}
\end{figure}

\subsection{Classification Figure of Merit}\label{cfm}
Literature: \cite{Hampshire1990}

The \gls{MSE} and the \gls{CE} functions focus on the difference of the network output $\mathbf{o}$ and target activations $\mathbf{t}$ and try to minimize it. \Gls{CFM} takes a different approach by only using the target activations to identify the correct class $c$ and then tries to maximize the difference between $o_c$ and all other $o_i (i\neq j)$.

\begin{equation}\label{eq:cfm}
E_{\text{CFM}} = \frac{1}{k-1} \sum_{i=1,i\neq c}^k \frac{\alpha}{1+e^{-\beta (o_c - o_i) + \gamma}}
\end{equation}

Where $c$ is the correct class, $k$ the number of classes and $\alpha$, $\beta$ and $\gamma$ are just parameters of the sigmoid function (scaling, discontinuity and lateral shift).

\subsection{L1 and L2 Regularization}
Small weights tend to perform better and we can modify the error functions from above to penalize big weight by adding additional error terms.
\begin{itemize}
\item L1 norm: $||\mathbf{w}||_{L1} = \sum_j |\mathbf{w}_j|$
\item L2 norm: $||\mathbf{w}||_{L2} = \sum_j \mathbf{w}_j^2$
\item new error function $E' = E + \alpha_{L1} ||\mathbf{w}||_{L1} + \alpha_{L2} ||\mathbf{w}||_{L2}$
\item new hyperparameters $\alpha_{L1}$ and $\alpha_{L2}$, optimize on development set
\end{itemize}

\todo{Explain advantages and disadvantages, see \url{http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization}}
\section{Learning Rate}\label{sec:learning-rate}
Literature: \cite[Chapter 6.8]{Duda2000}

We use the learning rate $\eta$ in our weight updates: $$w_{ij} \leftarrow w_{ij} + \eta\, \delta_j a_i$$

There are several problems with a constant learning rate:
\begin{itemize}
\item large learning rate
	\begin{itemize}
	\item constantly jump past the minimum ($\Rightarrow$ no convergence)
	\item neurons may saturate
	\end{itemize}
\item small learning rate
	\begin{itemize}
	\item slow training
	\item may get stuck in local minimum
	\end{itemize}
\end{itemize}

And of course several ways to deat with it them:
\begin{itemize}
\item Predetermined piecewise constant learning rate\newline
	Use a predetermined sequence of $\eta_i$, increment $i$ after every iteration (epoch/batch)
\item Exponentially decaying learning rate
\item Performance Scheduling
\item Weight Dependent Learning Rate Methods (\eg Momentum, AdaGrad)
\end{itemize}

\subsection{Exponential Decaying Learning Rate}
We calculate a new learning rate $\eta(t)$ after each iteration (epoch/batch).
\begin{equation}\label{eq:exponential-decaying-eta}
\eta(t)=\alpha\eta(t-1)=\alpha^t\eta(0)
\end{equation}
With the parameter $\alpha \in (0,1)$ and we still have to define an initial learning rate $\eta(0)$.

\subsection{Performance Scheduling}
Measure the cross validation error and decrease the learning rate when the error stops improving.

\subsection{Momentum or Rprop}\label{sec:momentum}
Literature: \cite{Riedmiller1994}

Similar to a rock rolling down a hill we let our gradient get some momentum to speed up training while the direction (sign) of the gradient stays the same. We do so by modifying our weight update rule to,
\begin{align}\label{eq:momentum}
w_{ij} &\leftarrow w_{ij} + v_{ij}(t)\\
v_{ij}(t) &\coloneqq \alpha\, v_{ij}(t-1) - \eta\, \delta_j a_i^{(l-1)}
\end{align}
, with $\alpha$ as momentum factor.

\todo{Verify that \emph{Rprop} and \emph{Momentum} are really the same or add differences}

\subsection{AdaGrad}
Literature: \cite{Zeiler2012,Duchi2011,Dyer}

\begin{quote}
AdaGrad alters the weight update to adapt based on historical information, so that frequently occurring features in the gradients get small learning rates and infrequent features get higher ones.
\end{quote}

\begin{align}
g_{ij} &= \frac{\partial E}{\partial w_{ij}}\\
\eta_{ij}(t) &= \frac{\eta_0}{\sqrt{\sum_{\tau=1}^t g_{ij}(\tau)^2}}g_{ij}(t)\\
w_{ij}(t) &\leftarrow w_{ij}(t-1) - \eta_{ij}(t)\,g_{ij}(t)
\end{align}

This eliminates the hyperparameter $\eta$, but introduces an initial learning rate $\eta_0$ and has some more downsides \cite{Zeiler2012}. One way to improve it is the limit the window for the sum to size $w$.
\begin{equation}
\eta_{ij}(t) = \frac{\eta_0}{\sqrt{\sum_{\tau=1}^w g_{ij}(t-\tau)^2}}g_{ij}(t)\\
\end{equation}

\emph{AdaDelta} contains further improvements.

\subsection{Newbob Scheduling}
Combination of exponential decaying learning rate and performance scheduling. Easy to implement and solid results.
\begin{itemize}
\item Start with constant learning rate $\eta(0)$
\item Once the cross-validation error stops decreasing switch to exponential decaying learning rate
\item End training after cross-validation error stops decreasing again
\end{itemize}
